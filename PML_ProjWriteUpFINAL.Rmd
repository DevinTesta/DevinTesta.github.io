---
title: "Analysis of Weight Lifting Exercise Dataset"
header-includes: \twocolumn
output:
  html_document: default
  pdf_document:
    fig_height: 3
    fig_width: 3
geometry: margin=.6in
---
```{r echo=FALSE, warnings=FALSE, messages=FALSE}
# load saved models to avoid lengthy reprocessing
model.Fit.gbm <- readRDS("modelFitgbm_01.rds")
model.Fit.CVone <- readRDS("modelFitCVone_01.rds")
model.Fit.One <- readRDS("modelFit1_01.rds")
```

\small

_We examined data from a published study "Qualitative Activity Recognition of Weight Lifting Exercises" by Vellose et al[1].  Several machine learning models were developed to predict outcomes on the Weight Lifting Exercises Dataset published as part of the study._

## Introduction

Study participants were asked to perform repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes. 

For data recording, the study authors used four 9 degrees of freedom Razor inertial measurement units (IMU), which provide three-axes
acceleration, gyroscope and magnetometer data. Sensors were mounted in participants' gloves, armbands, lumbar belts and dumbbells. Feature extraction from sensor data was developed by using a sliding window approach with different lengths from 0.5 second to 2.5 seconds, with
0.5 second overlap. In each step of the sliding window approach, the authors calculated features on the Euler angles (roll, pitch and yaw), as well as the raw accelerometer, gyroscope and magnetometer readings. For the Euler angles of each of the four sensors the authors calculated eight features: mean, variance, standard deviation, max, min, amplitude, kurtosis and skewness. 

This data set was used as the starting point for this machine learning project.

## Data Preparation

The data was downloaded from the Johns Hopkins/Coursera Practical Machine Learning course site.

```{r, echo=TRUE, message = FALSE, warning = FALSE}
## load libraries
library(caret)
library(randomForest)
library(pgmm)
library(foreach)

setInternet2(use = TRUE)  # for Windows environment
```

```{r}
## load data
train.data.URL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
download.file(train.data.URL, destfile="./rawTrainPML.csv")
 
test.data.URL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(test.data.URL, destfile="./rawTestPML.csv")

raw.train <- read.csv("rawTrainPML.csv",  stringsAsFactors = FALSE)  # this is the raw training data
raw.test <- read.csv("rawTestPML.csv",  stringsAsFactors = FALSE)  # this is the raw test data

```

The raw data was processed into training and testing sets. The following steps were performed:

1. The "classe" variable was set to be a factor variable in the training set. Since "classe" is the outcome variable being predicted, it is not in the testing data set.
2. Testing data had variables consisting entirely of NA values. These variables were removed from training and test data.
3. Label data were removed from the training set.
4. Training data in integer form were converted to numeric.
5. Training data observations were checked for completeness.

```{r}
set.seed(3456)

# The goal of the project is to predict the manner in which the participants did the exercise. 
# This is the "classe" variable 

raw.train$classe <- as.factor(raw.train$classe)

## identify variables with 'NA' values in the test data 
test.NA.cols <- which(sapply(raw.test, function(x) all(is.na(x))))

## these variables can be removed from train data and the testing data
training <- raw.train[,-test.NA.cols, drop = FALSE ]
testing <- raw.test[,-test.NA.cols, drop = FALSE ]

## remove label data from training set
training <- training[,-c(1:7)]

## convert integers to numeric type
integer.colnum <- which(sapply(training, function(x) all(is.integer(x))))
temp.df1 <- training[,-integer.colnum]
temp.df2 <- data.frame(sapply(training[,integer.colnum],as.numeric))
training <- cbind(temp.df1, temp.df2)

## check that rows are complete cases
complete.ind <- complete.cases(training)  # complete rows occur when "new_window" == "yes"
training <- training[complete.ind,]
```

The prepared training data set had `r dim(training)[1]` observations of `r dim(training)[2]-1` predictors and the outcome "classe".  

## Initial Review of Training Data

Spatial data of the type recorded by the sensors could be highly covariant. A correlation matrix of predictor variables confirms this.

```{r, eval=TRUE}
## check correlated predictors
df.cor <- subset(training, select = -c(classe))
M <- abs(cor(df.cor))   # create a matrix of correlations
diag(M) <- 0                    # set matching variables to zero correlation
which(M > 0.95, arr.ind = T)
```

For this reason, principal component analysis was considered as an option during model construction.

## Model Construction

Random forests, bagging, boosting, and other types of "tree" algorithms are candidates for modeling multi-category outcomes with weak predictors, which is the type of question posed for this project. Additionally, in their work the study authors used random forests and bagging algorithms to evaluate their data sets, due to the characteristic noise in the sensor data. Therefore, these methods were used as a starting point for this analysis.

### Stochastic Gradient Boosting Trial

The first model constructed used Stochastic Gradient Boosting as the method to fit the "classe"" variable. Eighty percent (80%) of the data set was retained for cross-validation. Additionally, a 10-fold cross-validation was repeated three times during learning.

```{r, eval=TRUE}
#####################################################################
## RESAMPLING MODEL WITH GBM
#####################################################################

myTrainCntl = trainControl(method = "repeatedcv", number = 10, 
                         repeats = 3, verboseIter = FALSE, 
                         returnResamp = "all")

# set up data
set.seed(3456)
makeCV.ind <- createDataPartition(training$classe, p=.8, list=F)
training.use <- training[makeCV.ind,]
training.cv <- training[-makeCV.ind,]
```
```{r, eval=FALSE}
model.Fit.gbm <- train(classe ~ ., method="gbm", data = training.use, verbose=FALSE, trControl = myTrainCntl)
saveRDS(model.Fit.gbm, "modelFitgbm_01.rds") # save a copy of the model
```

The results of this model were promising. This model was highly accurate in predicting the classifier in the cross-validation data set.

```{r, eval=TRUE, message = FALSE}
pred.gbm <- predict(model.Fit.gbm, newdata = training.cv)
confusionMatrix(pred.gbm, training.cv$classe)
```

The out of sample error for this model is estimated to be `r confusionMatrix(pred.gbm, training.cv$classe)$overall[1]`.

### Random Forests Trial

The second model constructed used the Random Forests algorithm to fit the "classe" variable. Again, eighty percent (80%) of the data set was retained for cross-validation. According to Breiman and Cutler[2], in random forests there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. It is estimated internally, during the run. Nevertheless, a cross-validation data set was used to compare the accuracy of this model against the Stochastic Gradient Boosting Model developed earlier.

```{r, eval=TRUE, message = FALSE}
# set environment for parallel processing in windows
library(doSNOW)

# data to process
set.seed(3456)
makeCV.ind <- createDataPartition(training$classe, p=.8, list=F)
training.use <- training[makeCV.ind,]
training.cv <- training[-makeCV.ind,]
x.rf <- subset(training.use, select = -c(classe))

# set up parallel computing environment
cl <- makeCluster(2)  # for 2 CPU cores
registerDoSNOW(cl)    # launch backend
```
```{r, eval=FALSE}
model.Fit.CVone <- foreach(ntree=rep(250,2), .combine=combine, .packages="randomForest") %dopar% {
                    randomForest(x.rf, training.use$classe,
                                 ntree=ntree,
                                 do.trace=FALSE,
                                 importance=TRUE)
              }

stopCluster(cl)
registerDoSEQ()
gc(reset=TRUE)

saveRDS(model.Fit.CVone, "modelFitCVone_01.rds")   # save a copy of the model
```

The results of this model also indicated a high degree of accuracy in predicting the classifications in the cross-validation data set.

```{r}
model.Fit.CVone
pred.CV <- predict(model.Fit.CVone, newdata = training.cv)
confusionMatrix(pred.CV, training.cv$classe)

```

The accuracy of this model using the cross-validation training data set was `r confusionMatrix(pred.CV, training.cv$classe)$overall[1]`. This result is even higher than the comparable result for the Stochastic Gradient Boosting model, but the potential for overfitting is noted with such high accuracy.

### Random Forests with PCA Trial

A third model was constructed using Random Forests, but in this model, the entire training data set was used, and Principal Component Analysis pre-processing was employed.

```{r}
x.PP <- subset(training, select = -c(classe))
preProcess.default(x = x.PP, method = "pca")
```

Note that pre-processing using PCA would require 25 components to capture 95 percent of the variance.

```{r, eval=FALSE}
model.Fit.1 <- train(classe ~ ., method="rf", preProcess = "pca", data = training)

saveRDS(model.Fit.1, "modelFitOne_01.rds")  # save model for later use
```

The accuracy of this model was not estimated using the cross-validation training data set, although it was expected to be similar to the previous Random Forests model. 

## Prediction Results

An ensemble of the three models was assembled to compare results using the test data set. The results were in agreement, except for one outlier.

```{r, eval=TRUE}
pred.test.rfPCA <- predict(model.Fit.One, newdata = testing)
pred.test.rfCV <- predict(model.Fit.CVone, newdata = testing)
pred.test.gbm <- predict(model.Fit.gbm, newdata = testing)
preds.all.df <- data.frame(RFwithPCA = pred.test.rfPCA, RFwithCV = pred.test.rfCV, GBM = pred.test.gbm)
preds.all.df
```

A majority vote was used to determine which result to use for the final predicted answer.

```{r, eval=TRUE}
MajorityVoteChr <- function(inputChrVec) {
      factorVec <- factor(inputChrVec) 
      A <- tabulate(factorVec)
      levels(factorVec)[which.max(A)]
}

for (r in seq_along(preds.all.df[,1])){
      df.eval <- t(preds.all.df)
      preds.all.df$Best[r] <- MajorityVoteChr(df.eval[,r])
}
```

This ensemble model resulted in 100 percent accuracy when evaluated against the test data classifier.

## Sources

1. Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.  Retrieved October 11, 2015 from http://groupware.les.inf.puc-rio.br/har

2. Random forests. (2015, October 13). Retrieved from http://www.math.usu.edu/~adele/forests/cc_home.htm#ooberr
